name: Backfill Historical Sessions

on:
  workflow_dispatch:
    inputs:
      sessions:
        description: 'Comma-separated session numbers (e.g., 71,72,73)'
        required: true
        default: '71,72,73,74,75,76,77'
      max_misses:
        description: 'Stop after N consecutive 404s per session'
        required: false
        default: '10'

permissions:
  contents: write

concurrency:
  group: backfill-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  backfill:
    name: Backfill sessions
    runs-on: ubuntu-latest
    timeout-minutes: 360
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          pip install -e .

      - name: Ensure branch checkout
        run: git checkout -B "${{ github.ref_name }}"

      - name: Pull latest
        run: git pull --rebase origin "${{ github.ref_name }}" || true

      - name: Download, extract, detect, link all sessions
        run: |
          set -e
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          IFS=',' read -ra SESSIONS <<< "${{ github.event.inputs.sessions }}"
          MAX_MISSES="${{ github.event.inputs.max_misses }}"
          SUMMARY=""

          for SESSION in "${SESSIONS[@]}"; do
            SESSION=$(echo "$SESSION" | tr -d ' ')
            echo "============================================"
            echo "ðŸ“¥ Session $SESSION"
            echo "============================================"

            # 1. Download
            echo "::group::Download session $SESSION"
            PDF_BEFORE=$(ls -1 data/pdfs/A_RES_${SESSION}_*.pdf 2>/dev/null | wc -l || echo 0)

            python -m mandate_pipeline.cli download-session \
              --session "$SESSION" \
              --config ./config \
              --data ./data \
              --max-misses "$MAX_MISSES" \
              --verbose || true

            PDF_AFTER=$(ls -1 data/pdfs/A_RES_${SESSION}_*.pdf 2>/dev/null | wc -l || echo 0)
            NEW_PDFS=$((PDF_AFTER - PDF_BEFORE))
            echo "Session $SESSION: $NEW_PDFS new PDFs ($PDF_AFTER total)"
            echo "::endgroup::"

            # Check if pipeline already completed for this session
            LINKED_COUNT=$(ls -1 data/linked/A_RES_${SESSION}_*.json 2>/dev/null | wc -l || echo 0)
            if [ "$NEW_PDFS" -eq 0 ] && [ "$LINKED_COUNT" -ge "$PDF_AFTER" ] && [ "$PDF_AFTER" -gt 0 ]; then
              echo "Session $SESSION already fully processed ($LINKED_COUNT linked files)"
              SUMMARY="${SUMMARY}| ${SESSION} | 0 | already done |\n"
              continue
            fi

            if [ "$PDF_AFTER" -eq 0 ]; then
              echo "No PDFs for session $SESSION, skipping"
              SUMMARY="${SUMMARY}| ${SESSION} | 0 | no PDFs |\n"
              continue
            fi

            # 2. Extract
            echo "::group::Extract session $SESSION"
            python -c "
          import json, time
          from pathlib import Path
          import sys
          sys.path.insert(0, 'src')
          from mandate_pipeline.extractor import extract_text, extract_operative_paragraphs, extract_title, extract_agenda_items, find_symbol_references

          session = '${SESSION}'
          pdfs = sorted(Path('data/pdfs').glob(f'A_RES_{session}_*.pdf'))
          extracted_dir = Path('data/extracted')
          extracted_dir.mkdir(parents=True, exist_ok=True)

          processed = 0
          for pdf_path in pdfs:
              out = extracted_dir / f'{pdf_path.stem}.json'
              if out.exists():
                  continue  # Already extracted
              try:
                  text = extract_text(pdf_path)
                  paragraphs = extract_operative_paragraphs(text)
                  title = extract_title(text)
                  agenda_items = extract_agenda_items(text)
                  symbol_refs = find_symbol_references(text)
                  extracted = {
                      'symbol': pdf_path.stem.replace('_', '/'),
                      'filename': pdf_path.name,
                      'title': title,
                      'text': text,
                      'paragraphs': paragraphs,
                      'agenda_items': agenda_items,
                      'symbol_references': symbol_refs,
                  }
                  with open(out, 'w') as f:
                      json.dump(extracted, f, indent=2)
                  processed += 1
              except Exception as e:
                  print(f'Error: {pdf_path.name}: {e}')

          print(f'Extracted {processed} new documents for session {session}')
          "
            echo "::endgroup::"

            # 3. Detect
            echo "::group::Detect session $SESSION"
            python -c "
          import json
          from pathlib import Path
          import sys
          sys.path.insert(0, 'src')
          from mandate_pipeline.detection import load_checks, run_checks

          session = '${SESSION}'
          checks = load_checks(Path('config/checks.yaml'))
          extracted_dir = Path('data/extracted')
          detected_dir = Path('data/detected')
          detected_dir.mkdir(parents=True, exist_ok=True)

          processed = 0
          for ef in sorted(extracted_dir.glob(f'A_RES_{session}_*.json')):
              out = detected_dir / ef.name
              if out.exists():
                  continue
              try:
                  with open(ef) as f:
                      doc = json.load(f)
                  paragraphs = doc.get('paragraphs', {})
                  signals = run_checks(paragraphs, checks)
                  signal_summary = {}
                  for para_signals in signals.values():
                      for sig in para_signals:
                          signal_summary[sig] = signal_summary.get(sig, 0) + 1
                  detected = {
                      'symbol': doc['symbol'],
                      'doc_type': 'resolution',
                      'signals': signals,
                      'signal_summary': signal_summary,
                  }
                  with open(out, 'w') as f:
                      json.dump(detected, f, indent=2)
                  processed += 1
              except Exception as e:
                  print(f'Error: {ef.name}: {e}')

          print(f'Detected signals in {processed} documents for session {session}')
          "
            echo "::endgroup::"

            # 4. Link
            echo "::group::Link session $SESSION"
            python -c "
          import json
          from pathlib import Path
          import sys
          sys.path.insert(0, 'src')

          session = '${SESSION}'
          detected_dir = Path('data/detected')
          linked_dir = Path('data/linked')
          linked_dir.mkdir(parents=True, exist_ok=True)

          processed = 0
          for df in sorted(detected_dir.glob(f'A_RES_{session}_*.json')):
              out = linked_dir / df.name
              if out.exists():
                  continue
              try:
                  with open(df) as f:
                      doc = json.load(f)
                  linked = {
                      'symbol': doc['symbol'],
                      'doc_type': doc.get('doc_type', 'resolution'),
                      'origin': 'Unknown',
                      'linked_proposals': [],
                      'linked_resolutions': [],
                      'signal_summary': doc.get('signal_summary', {}),
                      'signals': doc.get('signals', {}),
                  }
                  with open(out, 'w') as f:
                      json.dump(linked, f, indent=2)
                  processed += 1
              except Exception as e:
                  print(f'Error: {df.name}: {e}')

          print(f'Linked {processed} documents for session {session}')
          "
            echo "::endgroup::"

            SUMMARY="${SUMMARY}| ${SESSION} | ${NEW_PDFS} | âœ… |\n"

            # Commit after each session
            git add data/
            if ! git diff --staged --quiet; then
              git commit -m "chore: backfill session $SESSION â€” download, extract, detect, link"
              git pull --rebase origin "${{ github.ref_name }}" || true
              git push
              echo "âœ… Session $SESSION committed and pushed"
            fi
          done

          echo "SUMMARY<<EOF" >> $GITHUB_ENV
          echo -e "$SUMMARY" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV

      - name: Trigger generate
        run: |
          echo "Triggering generate workflow..."
          gh workflow run generate.yml --ref "${{ github.ref_name }}"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Summary
        if: always()
        run: |
          cat >> $GITHUB_STEP_SUMMARY << EOF
          ## Backfill Results

          | Session | New PDFs | Status |
          |---------|----------|--------|
          ${SUMMARY}
          EOF
