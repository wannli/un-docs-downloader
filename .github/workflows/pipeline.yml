name: Mandate Pipeline

on:
  workflow_dispatch:
    inputs:
      stages:
        type: choice
        description: 'Which stages to run'
        options:
          - all
          - core
          - discover
          - extract
          - detect
          - link
          - generate
          - discover+extract+detect+link
          - extract+detect+link+generate
          - extract+detect+generate
          - igov
          - igov-sync
          - igov-signals
          - build-session
      session_number:
        type: string
        description: 'UN GA session number (for discover, igov-sync, igov-signals, build-session)'
        required: false
      session_label:
        type: string
        description: 'Override IGov session label (for igov-sync)'
        required: false
      max_misses:
        type: string
        description: 'Max 404s for build-session (default: 5)'
        required: false
        default: '5'
  schedule:
    - cron: '0 * * * *'
    - cron: '30 * * * *'
    - cron: '15 2 * * *'

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: pipeline-${{ github.ref_name }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.12'
  PIP_CACHE_DIR: ~/.cache/pip
  STAGES: ${{ inputs.stages || 'core' }}

jobs:
  discover:
    name: Discover
    runs-on: ubuntu-latest
    container: python:3.12-slim
    if: env.STAGES == 'all' || env.STAGES == 'core' || env.STAGES == 'discover' || env.STAGES == 'discover+extract+detect+link'
    steps:
      - name: Install git
        run: |
          apt-get update && apt-get install -y --no-install-recommends git

      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install dependencies
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          pip install --cache-dir ${{ env.PIP_CACHE_DIR }} -e .

      - name: Ensure branch checkout
        run: git checkout -B "${GITHUB_REF_NAME}"

      - name: Discover new documents
        if: inputs.session_number == ''
        run: python -m mandate_pipeline.cli discover --config ./config --data ./data --verbose

      - name: Discover historical session
        if: inputs.session_number != ''
        run: python -m mandate_pipeline.cli download-session --session ${{ inputs.session_number }} --config ./config --data ./data --verbose

      - name: Record discovery run metadata
        run: |
          cat > data/last_discover_run.json << 'EOF'
          {
            "run_id": "${{ github.run_id }}",
            "run_attempt": "${{ github.run_attempt }}",
            "event_name": "${{ github.event_name }}",
            "ref": "${{ github.ref }}",
            "ref_name": "${{ github.ref_name }}",
            "sha": "${{ github.sha }}",
            "actor": "${{ github.actor }}",
            "timestamp_utc": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "session_number": "${{ inputs.session_number || null }}"
          }
          EOF

      - name: Commit changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            if [ -n "${{ inputs.session_number }}" ]; then
              git commit -m "chore: discover A/RES/${{ inputs.session_number }} session documents"
            else
              git commit -m "chore: discover new documents ($(date -I))"
            fi
            git pull --rebase origin "${GITHUB_REF_NAME}" || true
            git push
          fi

  extract:
    name: Extract
    runs-on: ubuntu-latest
    container: python:3.12-slim
    needs: discover
    if: env.STAGES == 'all' || env.STAGES == 'core' || env.STAGES == 'extract' || env.STAGES == 'discover+extract+detect+link' || env.STAGES == 'extract+detect+link+generate' || env.STAGES == 'extract+detect+generate'
    steps:
      - name: Install git
        run: |
          apt-get update && apt-get install -y --no-install-recommends git

      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install dependencies
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          pip install --cache-dir ${{ env.PIP_CACHE_DIR }} -e .

      - name: Ensure branch checkout
        run: git checkout -B "${GITHUB_REF_NAME}"

      - name: Extract document content
        run: |
          python -c "
          import sys
          import os
          from pathlib import Path
          sys.path.insert(0, 'src')
          from mandate_pipeline.extractor import extract_text, extract_operative_paragraphs, extract_title, extract_agenda_items, find_symbol_references
          import json
          import subprocess

          print('Finding PDFs to process...')

          if os.environ.get('GITHUB_EVENT_NAME') == 'push':
              try:
                  result = subprocess.run(['git', 'diff', '--name-only', 'HEAD~1', 'HEAD', '--', 'data/pdfs/*.pdf'],
                                        capture_output=True, text=True, cwd='.')
                  changed_files = result.stdout.strip().split('\n') if result.stdout.strip() else []
              except:
                  changed_files = []
          else:
              pdf_dir = Path('data/pdfs')
              changed_files = [str(f) for f in pdf_dir.rglob('*.pdf')] if pdf_dir.exists() else []

          pdf_files = [f for f in changed_files if f.strip()]
          print(f'Found {len(pdf_files)} PDFs to process')

          if not pdf_files:
              print('No PDFs to process')
              exit(0)

          processed_count = 0
          for pdf_path_str in pdf_files:
              pdf_path = Path(pdf_path_str)
              if not pdf_path.exists():
                  print(f'PDF not found: {pdf_path}')
                  continue

              try:
                  print(f'Extracting: {pdf_path.name}')
                  text = extract_text(pdf_path)
                  paragraphs = extract_operative_paragraphs(text)
                  title = extract_title(text)
                  agenda_items = extract_agenda_items(text)
                  symbol_refs = find_symbol_references(text)

                  extracted = {
                      'symbol': pdf_path.stem.replace('_', '/'),
                      'filename': pdf_path.name,
                      'title': title,
                      'text': text,
                      'paragraphs': paragraphs,
                      'agenda_items': agenda_items,
                      'symbol_references': symbol_refs
                  }

                  output_file = Path('data/extracted') / f'{pdf_path.stem}.json'
                  output_file.parent.mkdir(parents=True, exist_ok=True)

                  with open(output_file, 'w') as f:
                      json.dump(extracted, f, indent=2)

                  print(f'Extracted to: {output_file}')
                  processed_count += 1

              except Exception as e:
                  print(f'Error processing {pdf_path}: {e}')
                  continue

          print(f'Successfully processed {processed_count} PDFs')
          "

      - name: Commit extraction results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/extracted/
          if git diff --staged --quiet; then
            echo "No extraction changes to commit"
          else
            git commit -m "chore: extract document content"
            git pull --rebase origin "${GITHUB_REF_NAME}" || true
            git push
          fi

  detect:
    name: Detect
    runs-on: ubuntu-latest
    container: python:3.12-slim
    needs: extract
    if: env.STAGES == 'all' || env.STAGES == 'core' || env.STAGES == 'detect' || env.STAGES == 'discover+extract+detect+link' || env.STAGES == 'extract+detect+link+generate' || env.STAGES == 'extract+detect+generate'
    steps:
      - name: Install git
        run: |
          apt-get update && apt-get install -y --no-install-recommends git

      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install dependencies
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          pip install --cache-dir ${{ env.PIP_CACHE_DIR }} -e .

      - name: Ensure branch checkout
        run: git checkout -B "${GITHUB_REF_NAME}"

      - name: Run signal detection
        run: |
          python -c "
          import sys
          import os
          from pathlib import Path
          sys.path.insert(0, 'src')
          from mandate_pipeline.detection import load_checks, run_checks
          import json
          import subprocess

          print('Finding extracted documents to process...')

          config_changed = False
          if os.environ.get('GITHUB_EVENT_NAME') == 'push':
              try:
                  result = subprocess.run(['git', 'diff', '--name-only', 'HEAD~1', 'HEAD'],
                                        capture_output=True, text=True, cwd='.')
                  if 'config/checks.yaml' in result.stdout:
                      config_changed = True
                      print('Config changed - running full detection on all documents')
              except:
                  pass

          if os.environ.get('GITHUB_EVENT_NAME') == 'push' and not config_changed:
              try:
                  result = subprocess.run(['git', 'diff', '--name-only', 'HEAD~1', 'HEAD', '--', 'data/extracted/*.json'],
                                        capture_output=True, text=True, cwd='.')
                  extracted_files = result.stdout.strip().split('\n') if result.stdout.strip() else []
              except:
                  extracted_files = []
          else:
              extracted_dir = Path('data/extracted')
              extracted_files = [str(f) for f in extracted_dir.glob('*.json')] if extracted_dir.exists() else []

          extracted_files = [f for f in extracted_files if f.strip()]
          print(f'Found {len(extracted_files)} extracted documents to process')

          if not extracted_files:
              print('No extracted documents to process')
              exit(0)

          checks = load_checks(Path('config/checks.yaml'))
          print(f'Loaded {len(checks)} signal definitions')

          processed_count = 0
          for extracted_file_str in extracted_files:
              extracted_file = Path(extracted_file_str)
              if not extracted_file.exists():
                  print(f'Extracted file not found: {extracted_file}')
                  continue

              try:
                  print(f'Detecting signals in: {extracted_file.name}')

                  with open(extracted_file) as f:
                      doc_data = json.load(f)

                  signals = run_checks(doc_data['paragraphs'], checks)

                  detection_result = {
                      'symbol': doc_data['symbol'],
                      'signals': signals,
                      'signal_summary': {}
                  }

                  if signals:
                      for para_signals in signals.values():
                          for signal in para_signals:
                              detection_result['signal_summary'][signal] = detection_result['signal_summary'].get(signal, 0) + 1

                  output_file = Path('data/detected') / f'{extracted_file.stem}.json'
                  output_file.parent.mkdir(parents=True, exist_ok=True)

                  with open(output_file, 'w') as f:
                      json.dump(detection_result, f, indent=2)

                  print(f'Detection results saved to: {output_file}')
                  print(f'Found signals: {list(detection_result[\"signal_summary\"].keys())}')
                  processed_count += 1

              except Exception as e:
                  print(f'Error processing {extracted_file}: {e}')
                  continue

          print(f'Successfully processed {processed_count} documents')
          "

      - name: Commit detection results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/detected/
          if git diff --staged --quiet; then
            echo "No detection changes to commit"
          else
            git commit -m "chore: detect signals in documents"
            git pull --rebase origin "${GITHUB_REF_NAME}" || true
            git push
          fi

  link:
    name: Link
    runs-on: ubuntu-latest
    container: python:3.12-slim
    needs: detect
    if: env.STAGES == 'all' || env.STAGES == 'core' || env.STAGES == 'link' || env.STAGES == 'discover+extract+detect+link' || env.STAGES == 'extract+detect+link+generate'
    steps:
      - name: Install git
        run: |
          apt-get update && apt-get install -y --no-install-recommends git

      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install dependencies
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          pip install --cache-dir ${{ env.PIP_CACHE_DIR }} -e .

      - name: Ensure branch checkout
        run: git checkout -B "${GITHUB_REF_NAME}"

      - name: Run document linking
        run: |
          python -c "
          import sys
          import json
          from pathlib import Path
          sys.path.insert(0, 'src')
          from mandate_pipeline.linking import derive_resolution_origin

          print('Running document linking...')

          detected_dir = Path('data/detected')
          linked_dir = Path('data/linked')
          linked_dir.mkdir(parents=True, exist_ok=True)

          documents = []
          if detected_dir.exists():
              for detected_file in detected_dir.glob('*.json'):
                  with open(detected_file) as f:
                      doc = json.load(f)
                      documents.append(doc)

          print(f'Loaded {len(documents)} detected documents')

          linked_documents = []

          for doc in documents:
              symbol = doc['symbol']
              doc_type = 'resolution' if 'RES' in symbol else 'proposal' if 'L.' in symbol else 'other'

              linked_doc = {
                  'symbol': symbol,
                  'doc_type': doc_type,
                  'origin': derive_resolution_origin({
                      'symbol': symbol,
                      'linked_proposal_symbols': []
                  }),
                  'linked_proposals': [],
                  'linked_resolutions': [],
                  'signal_summary': doc.get('signal_summary', {}),
                  'signals': doc.get('signals', {})
              }

              linked_documents.append(linked_doc)

              output_file = linked_dir / f'{Path(symbol.replace(\"/\", \"_\")).stem}.json'
              with open(output_file, 'w') as f:
                  json.dump(linked_doc, f, indent=2)

          linkage_index = {
              'documents': linked_documents,
              'stats': {
                  'total_documents': len(linked_documents),
                  'resolutions': len([d for d in linked_documents if d['doc_type'] == 'resolution']),
                  'proposals': len([d for d in linked_documents if d['doc_type'] == 'proposal']),
                  'linked_pairs': 0
              }
          }

          with open(linked_dir / 'index.json', 'w') as f:
              json.dump(linkage_index, f, indent=2)

          print(f'Created linkages for {len(linked_documents)} documents')
          print('Document types:', linkage_index['stats'])
          "

      - name: Commit linking results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/linked/
          if git diff --staged --quiet; then
            echo "No linking changes to commit"
          else
            git commit -m "chore: link related documents"
            git pull --rebase origin "${GITHUB_REF_NAME}" || true
            git push
          fi

  generate:
    name: Generate
    runs-on: ubuntu-latest
    container: python:3.12-slim
    needs: link
    if: env.STAGES == 'all' || env.STAGES == 'core' || env.STAGES == 'generate' || env.STAGES == 'extract+detect+link+generate' || env.STAGES == 'extract+detect+generate'
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Install git
        run: |
          apt-get update && apt-get install -y --no-install-recommends git

      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install dependencies
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          pip install --cache-dir ${{ env.PIP_CACHE_DIR }} -e .

      - name: Ensure branch checkout
        run: git checkout -B "${GITHUB_REF_NAME}"

      - name: Generate unified signals explorer
        env:
          SKIP_UNDL_METADATA: ${{ vars.SKIP_UNDL_METADATA || 'false' }}
          SKIP_DETAILED_PAGES: ${{ vars.SKIP_DETAILED_PAGES || 'true' }}
          MAX_DOCUMENTS: ${{ vars.MAX_DOCUMENTS || '' }}
        run: |
          python -c "
          import sys
          import os
          import json
          from pathlib import Path
          sys.path.insert(0, 'src')
          from mandate_pipeline.generation import generate_unified_explorer_page
          from mandate_pipeline.detection import load_checks

          print('Starting unified signals explorer generation...')

          linked_dir = Path('data/linked')
          documents = []

          if linked_dir.exists():
              print('Loading linked documents...')
              for linked_file in linked_dir.glob('*.json'):
                  if linked_file.name == 'index.json':
                      continue
                  try:
                      with open(linked_file) as f:
                          doc = json.load(f)
                          documents.append(doc)
                  except Exception as e:
                      print(f'Error loading {linked_file}: {e}')

          print(f'Loaded {len(documents)} linked documents')

          max_docs = os.getenv('MAX_DOCUMENTS')
          if max_docs and max_docs.isdigit():
              max_docs = int(max_docs)
              print(f'Limiting to {max_docs} documents for faster processing')
              documents = documents[:max_docs]

          if documents:
              checks = load_checks(Path('config/checks.yaml'))
              print(f'Loaded {len(checks)} checks')

              print('Generating unified signals explorer...')
              generate_unified_explorer_page(documents, checks, Path('docs'))

              print('Unified signals explorer generation completed!')
          else:
              print('No documents to generate unified explorer for')
          "

      - name: Commit site changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/
          if git diff --staged --quiet; then
            echo "No site changes to commit"
          else
            git commit -m "chore: generate static site"
            git pull --rebase origin "${GITHUB_REF_NAME}" || true
            git push
            echo "Site updated and committed to default branch"
          fi

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./docs

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

  igov-sync:
    name: IGov Sync
    runs-on: ubuntu-latest
    container: python:3.12-slim
    if: env.STAGES == 'all' || env.STAGES == 'igov' || env.STAGES == 'igov-sync' || github.event_name == 'schedule'
    steps:
      - name: Install git
        run: |
          apt-get update && apt-get install -y --no-install-recommends git

      - name: Checkout
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          pip install --cache-dir ${{ env.PIP_CACHE_DIR }} -e .

      - name: Sync IGov decisions
        run: |
          if [ -n "${{ inputs.session_number }}" ] && [ -n "${{ inputs.session_label }}" ]; then
            python -m mandate_pipeline.cli igov-sync \
              --session ${{ inputs.session_number }} \
              --session-label "${{ inputs.session_label }}" \
              --config ./config \
              --data ./data \
              --verbose
          elif [ -n "${{ inputs.session_number }}" ]; then
            python -m mandate_pipeline.cli igov-sync \
              --session ${{ inputs.session_number }} \
              --config ./config \
              --data ./data \
              --verbose
          else
            python -m mandate_pipeline.cli igov-sync --config ./config --data ./data --verbose
          fi

      - name: Record IGov sync metadata
        run: |
          SESSION_TAG="${{ inputs.session_number || 'current' }}"
          cat > data/igov/last_sync_run_${SESSION_TAG}.json << 'EOF'
          {
            "run_id": "${{ github.run_id }}",
            "run_attempt": "${{ github.run_attempt }}",
            "event_name": "${{ github.event_name }}",
            "ref": "${{ github.ref }}",
            "ref_name": "${{ github.ref_name }}",
            "sha": "${{ github.sha }}",
            "actor": "${{ github.actor }}",
            "timestamp_utc": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "session_number": "${{ inputs.session_number || '' }}",
            "session_label": "${{ inputs.session_label || '' }}"
          }
          EOF

      - name: Commit changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/igov
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            if [ -n "${{ inputs.session_number }}" ]; then
              git commit -m "chore: sync IGov decisions for session ${{ inputs.session_number }}"
            else
              git commit -m "chore: sync IGov decisions ($(date -I))"
            fi
            git pull --rebase origin "${GITHUB_REF_NAME}" || true
            git push
          fi

  igov-signals:
    name: IGov Signals
    runs-on: ubuntu-latest
    container: python:3.12-slim
    needs: igov-sync
    if: env.STAGES == 'all' || env.STAGES == 'igov' || env.STAGES == 'igov-signals' || github.event_name == 'schedule'
    steps:
      - name: Install git
        run: |
          apt-get update && apt-get install -y --no-install-recommends git

      - name: Checkout
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          pip install --cache-dir ${{ env.PIP_CACHE_DIR }} -e .

      - name: Generate IGov signal browser
        run: |
          if [ -n "${{ inputs.session_number }}" ]; then
            python -m mandate_pipeline.cli igov-signals \
              --session ${{ inputs.session_number }} \
              --config ./config \
              --data ./data \
              --output "docs/igov" \
              --verbose
          else
            python -m mandate_pipeline.cli igov-signals \
              --config ./config \
              --data ./data \
              --output "docs/igov" \
              --verbose
          fi

      - name: Commit changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/igov
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            if [ -n "${{ inputs.session_number }}" ]; then
              git commit -m "chore: update IGov signal browser for session ${{ inputs.session_number }}"
            else
              git commit -m "chore: update IGov signal browser ($(date -I))"
            fi
            git pull --rebase origin "${GITHUB_REF_NAME}" || true
            git push
          fi

  build-session:
    name: Build Session
    runs-on: ubuntu-latest
    container: python:3.12-slim
    if: (env.STAGES == 'all' || env.STAGES == 'build-session') && inputs.session_number != ''
    steps:
      - name: Install git
        run: |
          apt-get update && apt-get install -y --no-install-recommends git

      - name: Checkout
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          pip install --cache-dir ${{ env.PIP_CACHE_DIR }} -e .

      - name: Build session
        run: |
          python -m mandate_pipeline.cli build-session \
            --session ${{ inputs.session_number }} \
            --config ./config \
            --data ./data \
            --output ./docs \
            --max-misses ${{ inputs.max_misses || '5' }} \
            --verbose

      - name: Commit changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/sessions/
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "chore: build session ${{ inputs.session_number }} signal browser ($(date -I))"
            git pull --rebase origin "${GITHUB_REF_NAME}" || true
            git push
          fi
